{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value():\n",
    "    def __init__(self,data,_children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self,other),'+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0*out.grad\n",
    "            other.grad += 1.0*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return -1*self\n",
    "    \n",
    "    def __sub__(self,other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self,other):\n",
    "        return self+other\n",
    "\n",
    "    def __rsub__(self,other):\n",
    "        return self-other\n",
    "    \n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other,Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self,other),\"*\")\n",
    "        def _backward():\n",
    "            self.grad += other.data*out.grad\n",
    "            other.grad += self.data*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self,other):\n",
    "        return self*other\n",
    "    \n",
    "    def __pow__(self,other):\n",
    "        assert isinstance(other,(int,float)), \"Power can only be int or float\"\n",
    "        out = Value(self.data**other,(self,),f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other*(self.data**(other-1))*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __truediv__(self,other):\n",
    "        return self * other**-1\n",
    "    def __rtruediv__(self,other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n",
    "        out = Value(t,(self,),'tanh')\n",
    "        def _backwards():\n",
    "            self.grad += (1 - t**2)*out.grad\n",
    "        out._backward = _backwards\n",
    "        return out\n",
    "    \n",
    "    def expo(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x),(self,),'exp')\n",
    "        def _backwards():\n",
    "            self.grad += out.data*out.grad\n",
    "        out._backward = _backwards\n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    build_topo(child)\n",
    "            topo.append(node)\n",
    "        build_topo(self)\n",
    "    \n",
    "        self.grad = 1\n",
    "        for node in topo[::-1]:\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value(data=0.7615941559557649)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = Value(0)\n",
    "w1 = Value(2)\n",
    "x2 = Value(3)\n",
    "w2 = Value(1)\n",
    "b = Value(-2)\n",
    "\n",
    "x1w1 = x1*w1\n",
    "x2w2 = x2*w2\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2\n",
    "\n",
    "n=x1w1x2w2+b\n",
    "print(n)\n",
    "o=n.tanh()\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2599230248420783"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.6666666666666666)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1/x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self,nin):\n",
    "        self.w = [Value(np.random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(np.random.uniform(-1,1))\n",
    "    def __call__(self,x):\n",
    "        act = self.b\n",
    "        for wi, xi in zip(self.w,x):\n",
    "            act += wi*xi\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,nin,nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    def __call__(self, x):\n",
    "        self.outs = [n(x) for n in self.neurons]\n",
    "        return self.outs\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            ps = neuron.parameters()\n",
    "            params.extend(ps)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,nin: int,nout: list):\n",
    "        sz = [nin]+nout\n",
    "        self.layers = [Layer(sz[i],sz[i+1]) for i in range(len(nout))]\n",
    "    def __call__(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            ps = layer.parameters()\n",
    "            params.extend(ps)\n",
    "        return params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.619063536961183)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = MLP(2,[6,6,1])\n",
    "x = [2.0,3.0]\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Value(data=0.619063536961183)],\n",
       " [Value(data=0.5770782571215954)],\n",
       " [Value(data=0.8229916296568681)],\n",
       " [Value(data=0.5540421550675353)]]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [\n",
    "    [2.0,3.0,-1.0],\n",
    "    [3.0,-1.0,0.5],\n",
    "    [-0.5,1.0,1.0],\n",
    "    [1.0,1.0,-1.0]\n",
    "]\n",
    "\n",
    "ys = [1.0,-1.0,-1.0,1.0]\n",
    "\n",
    "y_pred = [n(x) for x in xs]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=6.154465299214025)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum([(y_out[0]-y_gt)**2 for y_gt, y_out in zip(ys,y_pred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.144115527738787"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.975122772555551\n",
      "1 3.55305032975972\n",
      "2 2.7304789319151346\n",
      "3 3.2893711925236455\n",
      "4 2.7312368489187895\n",
      "5 2.402979517308912\n",
      "6 1.5203235988438772\n",
      "7 2.287529603287638\n",
      "8 3.4446445113228856\n",
      "9 1.2723577290818076\n",
      "10 1.648139220437081\n",
      "11 3.5325541211190528\n",
      "12 0.36235570013920193\n",
      "13 0.19238038769802746\n",
      "14 0.1467778509633363\n",
      "15 0.11814548240011431\n",
      "16 0.09896799740732573\n",
      "17 0.08599086162865391\n",
      "18 0.07544524813417687\n",
      "19 0.06752425646075655\n",
      "20 0.0609211119969099\n",
      "21 0.055455213342794005\n",
      "22 0.050938248561463714\n",
      "23 0.04710755197649699\n",
      "24 0.04378339801266237\n",
      "25 0.04078401385634208\n",
      "26 0.03819933977896019\n",
      "27 0.03586344669214102\n",
      "28 0.033768101032954786\n",
      "29 0.03195358933998554\n",
      "30 0.03026657223708513\n",
      "31 0.028771885151291456\n",
      "32 0.02741769834573055\n",
      "33 0.026145880250917467\n",
      "34 0.024980222292506443\n",
      "35 0.02390757512727952\n",
      "36 0.022942055216708078\n",
      "37 0.02202534748273192\n",
      "38 0.021175371429981\n",
      "39 0.02039813169153088\n",
      "40 0.019661316248561374\n",
      "41 0.018973559972636205\n",
      "42 0.018328253092745085\n",
      "43 0.0177325747876818\n",
      "44 0.01717721241295735\n",
      "45 0.016649038636027035\n",
      "46 0.01613970642875484\n",
      "47 0.015672106427700137\n",
      "48 0.015220640756427782\n",
      "49 0.014792810913611084\n",
      "50 0.014387416767135418\n",
      "51 0.01399931288290401\n",
      "52 0.013631696484235045\n",
      "53 0.01328875995150718\n",
      "54 0.012954730865871173\n",
      "55 0.012636485720762165\n",
      "56 0.012334245020868283\n",
      "57 0.012049957810552481\n",
      "58 0.01177954310379813\n",
      "59 0.011518368049032888\n",
      "60 0.011263230135728587\n",
      "61 0.011023242910704244\n",
      "62 0.010792813286403528\n",
      "63 0.010571035433810116\n",
      "64 0.01035796813901854\n",
      "65 0.010149698322100689\n",
      "66 0.009953458204294946\n",
      "67 0.00976432014894162\n",
      "68 0.009580712701083044\n",
      "69 0.009400202425676001\n",
      "70 0.00922943375838126\n",
      "71 0.009062068423677865\n",
      "72 0.008903406047929795\n",
      "73 0.008750173953172194\n",
      "74 0.008598655551386494\n",
      "75 0.008454896652261634\n",
      "76 0.00831580450437046\n",
      "77 0.008177363200788854\n",
      "78 0.008043189773247932\n",
      "79 0.007915787000653459\n",
      "80 0.007790431514640202\n",
      "81 0.007670459417993514\n",
      "82 0.0075538950942641596\n",
      "83 0.007438475277688372\n",
      "84 0.00732855908474103\n",
      "85 0.007222277486662494\n",
      "86 0.007118402858017892\n",
      "87 0.007017262408098284\n",
      "88 0.006918815783440025\n",
      "89 0.00682112648623332\n",
      "90 0.00672844379250597\n",
      "91 0.006635803816497138\n",
      "92 0.006547247598912551\n",
      "93 0.006461433595464528\n",
      "94 0.006375674220615154\n",
      "95 0.006293629082692383\n",
      "96 0.006212462515885352\n",
      "97 0.006132886263040127\n",
      "98 0.006055621903318271\n",
      "99 0.005981221070928228\n"
     ]
    }
   ],
   "source": [
    "n = MLP(2,[3,3,1])\n",
    "\n",
    "for k in range(100):\n",
    "    y_pred = [n(x) for x in xs]\n",
    "    loss = sum([(y_out[0]-y_gt)**2 for y_gt, y_out in zip(ys,y_pred)])\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0\n",
    "    loss.backward()\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "    print(k,loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Value(data=0.9684279602272388)],\n",
       " [Value(data=-0.9657066308767059)],\n",
       " [Value(data=-0.954075660064176)],\n",
       " [Value(data=0.9587768607315286)]]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
